{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae32743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tldextract\n",
    "import urllib\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/scratch/venia/web2wiki/helpers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5124ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/18 20:37:46 WARN Utils: Your hostname, iccluster039 resolves to a loopback address: 127.0.1.1; using 10.90.38.15 instead (on interface ens786f0)\n",
      "22/08/18 20:37:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/veselovs/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/18 20:37:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/18 20:37:48 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "22/08/18 20:37:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/08/18 20:37:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from settings import DATA_DIR, EMBEDDING_DIR\n",
    "\n",
    "\n",
    "os.environ['SPARK_HOME'] = \"/home/veselovs/spark-3.2.1-bin-hadoop2.7\"\n",
    "os.environ['JAVA_HOME'] = \"/home/veselovs/jdk-13.0.2\"\n",
    "\n",
    "config = pyspark.SparkConf().setAll([('spark.executor.memory', '50g'),\n",
    "                                 ('spark.executor.cores', '40'),\n",
    "                                 ('spark.driver.memory','50g'),\n",
    "                                 ('spark.driver.maxResultSize','0'),\n",
    "                                 ('spark.python.worker.memory', '5g'),\n",
    "                                 ('spark.reducer.maxSizeInFlight','5g'),\n",
    "                                 ('spark.rpc.message.maxSize', '1000'),\n",
    "                                 ('spark.sql.autoBroadcastJoinThreshold','-1'),\n",
    "                                 ('spark.local.dir', '/tmp/')])\n",
    "sc = pyspark.SparkContext(conf=config)\n",
    "spark = SparkSession.builder.appName('Venia').config(conf=config).getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dda115a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.reducer.maxSizeInFlight', '5g'),\n",
       " ('spark.driver.memory', '50g'),\n",
       " ('spark.rpc.message.maxSize', '1000'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/scratch/venia/web2wiki/iterative_coding/spark-warehouse'),\n",
       " ('spark.driver.port', '45807'),\n",
       " ('spark.local.dir', '/tmp/'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', '10.90.38.15'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.app.id', 'local-1660847870325'),\n",
       " ('spark.app.startTime', '1660847867976'),\n",
       " ('spark.executor.cores', '40'),\n",
       " ('spark.python.worker.memory', '5g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.maxResultSize', '0'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.autoBroadcastJoinThreshold', '-1'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.executor.memory', '50g')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc9ef2",
   "metadata": {},
   "source": [
    "- Curlie data\n",
    "- ORES topics\n",
    "- Contexts\n",
    "- Number of times the domain shares the article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80985a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def extract_subdomain(x):\n",
    "    if len(tldextract.extract(x).subdomain) > 1:\n",
    "        y = tldextract.extract(x).subdomain +\".\"+tldextract.extract(x).registered_domain\n",
    "    else:\n",
    "        y = tldextract.extract(x).registered_domain\n",
    "    return y\n",
    "\n",
    "@F.udf\n",
    "def extract_domain(x):\n",
    "    y = tldextract.extract(x).registered_domain\n",
    "    return y\n",
    "\n",
    "@F.udf\n",
    "def normalise_title(title):\n",
    "    \"\"\" Replace _ with space, remove anchor, capitalize \"\"\"\n",
    "    title = title.split(\"/\")[-1]\n",
    "    title = title.split(\"#\")[0]\n",
    "    title = urllib.parse.unquote(title)\n",
    "    title = title.strip()\n",
    "    if len(title) > 0:\n",
    "        title = title[0].upper() + title[1:]\n",
    "    n_title = title.replace(\"_\", \" \")\n",
    "    # if '#' in n_title:\n",
    "    #     n_title = n_title.split('#')[0]\n",
    "    return n_title\n",
    "\n",
    "\n",
    "def clean_web_content(web_content,drop_dups = True):\n",
    "    \"\"\"\n",
    "    Run this once... To generate the metadata file.\n",
    "    \"\"\"\n",
    "    web_content = web_content.dropna(subset = [\"url\",\"wiki_url\"])\n",
    "    if drop_dups == True:\n",
    "        web_content.dropDuplicates(subset=[\"url\",\"wiki_url\"])\n",
    "    web_content=web_content.withColumn(\"title\", normalise_title(\"wiki_url\"))\n",
    "    web_content = web_content.withColumn(\"subdomain\", extract_subdomain(\"url\"))\n",
    "    web_content = web_content.withColumn(\"domain\", extract_domain(\"url\"))\n",
    "    web_content = web_content.filter(F.col(\"title\") != \"\")\n",
    "    return web_content\n",
    "\n",
    "\n",
    "# this fn is necessary for reversing the host names\n",
    "@F.udf \n",
    "def reverse_reverse_host(x):\n",
    "    x = x.split(\".\")\n",
    "    x = x[-1::-1]\n",
    "    x = \".\".join(x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcd83025",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing files\n",
    "harmonic = True\n",
    "all_crawl = True\n",
    "wiki_mirror = False\n",
    "all_links = True\n",
    "\n",
    "\n",
    "web_content = spark.read.load(\"/scratch/venia/web2wiki/data/web_content/iterative_coding_sample/wiki_content.parquet\")\n",
    "web_content = clean_web_content(web_content)\n",
    "\n",
    "if wiki_mirror == True:\n",
    "    mirrors = spark.read.load(\"/scratch/venia/web2wiki/data/all_wikimirror_shares.parquet\")\n",
    "\n",
    "if harmonic == True:\n",
    "    \n",
    "    if all_crawl:\n",
    "        crawl_dump =spark.read.option(\"delimiter\", \"\\t\").csv(DATA_DIR + \"common_crawl_dump/cc-main-2021-feb-apr-may-domain-ranks.txt.gz\", header = True)\n",
    "        crawl_dump = crawl_dump.withColumn(\"domain\",reverse_reverse_host(\"#host_rev\"))\n",
    "    else:\n",
    "        crawl_dump = spark.read.load(\"/scratch/venia/web2wiki/data/top_domains_pr.parquet\")\n",
    "\n",
    "# en_shares = spark.read.load(\"/scratch/venia/web2wiki/data/en_shares_merged.parquet\")\n",
    "# en_curlie = spark.read.load(\"/scratch/venia/web2wiki/data/en_curlie_metadata.parquet\")\n",
    "    \n",
    "en_shares = pd.read_parquet(\"/scratch/venia/web2wiki/data/en_shares_merged2.parquet\")\n",
    "en_curlie = pd.read_parquet(\"/scratch/venia/web2wiki/data/en_curlie_metadata.parquet\")\n",
    "    \n",
    "\n",
    "# url counts, use this for limiting to sites that only share one url\n",
    "url_counts = spark.read.load(\"/scratch/venia/web2wiki/data/all_url_counts.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04b9862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(url='http://www.hadice-prumyslove.cz/p/3142/hadice-flexibilni-pur-lehka-ag-tl-07-mm-dn-165-mm', count=1),\n",
       " Row(url='http://www.rvb-hassberge.de/meine-bank/geschaeftsstellen/berater/berater-zeil.html', count=1),\n",
       " Row(url='http://www.pages.mi.it/2014/12/02/ied-201415-sociologia-della-comunicazione-visuale-parte-duetre', count=6)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_counts.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241da3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dist = url_counts.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a9aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70522ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "320f0d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1dc03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229cd07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55b0497a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87437436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if it's a bold, emphasize, ... then we will map it to the second level location. \n",
    "# web_content.loc[web_content[\"a1\"].isin(word_changes), \"a1\"] = web_content.loc[web_content[\"a1\"].isin(word_changes), \"a2\"]\n",
    "\n",
    "# # replace any hx with header\n",
    "# web_content.loc[web_content[\"a1\"].str.match(\"h[1-6]\"), \"a1\"] = \"header\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b93b71c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ensure english\n",
    "from langdetect import detect\n",
    "from whatthelang import WhatTheLang\n",
    "\n",
    "@F.udf\n",
    "def detect_lang(x):\n",
    "    wtl = WhatTheLang()\n",
    "    try: \n",
    "        return wtl.predict_lang(x)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def sample_files(web_content, with_lang = True, prop = 0.015):\n",
    "    web_content_sample = web_content.sample(prop)\n",
    "    if with_lang == True:\n",
    "        web_content_sample = web_content_sample.withColumn(\"lang\", detect_lang(\"text1\"))\n",
    "        web_content_sample = web_content_sample.filter(F.col(\"lang\") == \"en\")\n",
    "    return web_content_sample\n",
    "\n",
    "web_content_sample = sample_files(web_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8df4fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def extract_wiki_text(row):\n",
    "    bs = BeautifulSoup(row,\"xml\")\n",
    "    text = bs.get_text()\n",
    "    if len(text)< 2:\n",
    "        text = \"None\"\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d530e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7b5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f70a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will also sample the explicit mentions of Wikipedia\n",
    "web_content = web_content_sample.withColumn(\"text\", extract_wiki_text(F.col(\"text1\")))\n",
    "# sample_explicit = sample_files(a, with_lang= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03805654",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_content = web_content.join(crawl_dump, on =\"domain\",how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "736fcb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "web_content_sample = web_content.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55addc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_content_sample[\"#pr_pos\"] = web_content_sample[\"#pr_pos\"].astype(int)\n",
    "top_pr = web_content_sample.sort_values(by = \"#pr_pos\").head(2000).drop_duplicates(subset = [\"domain\"]).sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4c5348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fa5703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e12cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42da024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ad99894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_articles = ['Cloud computing', 'ISO 3166-1 alpha-2', 'WordPress', 'oronavirus disease 2019',\n",
    "       'Dunning–Kruger effect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "545c7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class sample_articles():\n",
    "    \"\"\"\n",
    "    This class is used to call various sampling techniques for \n",
    "    sampling the articles for the iterative coding. \n",
    "    \"\"\"\n",
    "    def __init__(self, web_contentt):\n",
    "        \"\"\"\n",
    "        web_contentt: this is the cleaned web2wiki content. \n",
    "        \"\"\"\n",
    "        self.web_content = web_contentt.copy()\n",
    "        self.sampled = None\n",
    "        self.sampled_html = None\n",
    "        \n",
    "    def map_html_structure(self, word_changes):\n",
    "        web_content = self.web_content\n",
    "        web_content.loc[web_content[\"a1\"].isin(word_changes), \"a1\"] = web_content.loc[web_content[\"a1\"].isin(word_changes), \"a2\"]\n",
    "        web_content.loc[web_content[\"a1\"].str.match(\"h[1-6]\"), \"a1\"] = \"header\"\n",
    "        return web_content\n",
    "\n",
    "\n",
    "    def article_sample(self, sample_col = \"title\", buckets = 5, sample_number = 15):\n",
    "        \"\"\"\n",
    "        This file samples articles based on buckets.\n",
    "        We drop duplicates on domains to get many different domains that invoke articles\n",
    "        instead of the dominant several. \n",
    "        \n",
    "        sample_col: this will be the column that we use to define buckets over\n",
    "        buckets: determines the number of buckets we use to sample over\n",
    "        sample_number: how many samples we want from each bucket. \n",
    "        \"\"\"\n",
    "        assert self.sampled == None, f\"Already sampled\"\n",
    "        web_content = self.web_content\n",
    "        web_content[\"article_count\"] = web_content.groupby([sample_col])[sample_col].transform(\"count\")\n",
    "        web_content[\"bucket\"] = pd.cut(web_content[\"article_count\"].rank(pct=True), buckets, labels=False)\n",
    "        web_content = web_content.drop_duplicates(subset = [\"domain\"])\n",
    "        sampled = pd.DataFrame()\n",
    "        for i in range(buckets):\n",
    "            sampled = sampled.append(web_content[web_content[\"bucket\"] == i].sample(sample_number))\n",
    "    \n",
    "        self.sampled = sampled\n",
    "        return sampled\n",
    "    \n",
    "    def sample_html_structure(self, tags,sample_number = 7, word_changes = None):\n",
    "        \"\"\"\n",
    "        Sample over the HTML structure.\n",
    "        For each of the in tags\n",
    "        \n",
    "        tags: a list of HTML tags we wish to restrict to        \n",
    "        sample_number: the number of rows we wish to sample\n",
    "        word_changes: sometimes, a1 may be a bold or some other feature, in this case we want to look up one level.\n",
    "        \"\"\"\n",
    "        assert self.sampled_html == None, f\"Already sampled\"\n",
    "        if word_changes != None:\n",
    "            web_content = map_html_structure(word_changes)\n",
    "        else:\n",
    "            web_content = self.web_content\n",
    "        \n",
    "        web_content = web_content[web_content[\"a1\"].isin(tags)]\n",
    "        tag_count = web_content.groupby(\"a1\")[\"title\"].count().reset_index()\n",
    "        tag_count = tag_count[tag_count[\"title\"] > 9]\n",
    "        web_content = web_content.drop_duplicates(subset = [\"domain\"])\n",
    "        web_content= web_content[web_content[\"a1\"].isin(tag_count.a1)]\n",
    "        sampled = web_content.groupby(\"a1\").sample(sample_number, replace = True)\n",
    "        sampled =  sampled.drop_duplicates()\n",
    "        \n",
    "        self.sampled_html= sampled\n",
    "\n",
    "        return sampled, tag_count\n",
    "    \n",
    "    def list_append(self, subset_articles, sample_number = 5):\n",
    "        \"\"\"\n",
    "        We sample one article many times to see the different contexts it can arise in.\n",
    "        \n",
    "        subset_articles: a list of articles that we want to sample over.\n",
    "        \"\"\"\n",
    "        web_content = self.web_content\n",
    "        web_content = web_content[web_content[\"title\"].isin(subset_articles)]\n",
    "        web_content = web_content.drop_duplicates(subset = \"domain\")\n",
    "        appended_vals = web_content.groupby(\"title\").sample(sample_number)\n",
    "        return appended_vals\n",
    "\n",
    "\n",
    "# sample,wc2 = article_sample(web_content, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d67e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sa = sample_articles(web_content_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb4725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e14818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d3b4840",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = sa.list_append(subset_articles).sample(frac = 1)\n",
    "sample2 = sa.article_sample().sample(frac= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c03cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c5375c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25bd7e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_neighbouring_text(x,text, num_chars = 300):\n",
    "        \"\"\"\n",
    "        Extracts the neighbouring text of the Wikipedia mention\n",
    "        \"\"\"\n",
    "        ind = x.find(text)\n",
    "        n = len(x)\n",
    "\n",
    "        if ind - num_chars < 0:\n",
    "            ind_0 = 0\n",
    "        else: ind_0 = ind - 200\n",
    "        if ind + num_chars > n:\n",
    "            return x[ind_0:]\n",
    "        else: \n",
    "            ind_1 = ind + num_chars + 50\n",
    "            return x[ind_0:ind_1]\n",
    "\n",
    "\n",
    "        return x[ind_0:ind_1]\n",
    "\n",
    "\n",
    "\n",
    "def process_sample(sample, harmonic = False):\n",
    "    \"\"\"\n",
    "    This will process a sample to make it clean for iterative coding\n",
    "    \n",
    "    sample: this is the sample of shares we want to process\n",
    "    harmonic: if we include harminoc position\n",
    "    \"\"\"   \n",
    "    sample = sample.merge(en_shares, on = \"title\")\n",
    "    sample[\"context\"] = sample.apply(lambda x: extract_neighbouring_text(x[\"text2\"], x[\"wiki_url\"]), axis = 1)\n",
    "    if harmonic == True:\n",
    "        sample = sample[[\"url\", \"domain\",\"subdomain\",\"#harmonicc_pos\",\"a1\", \"a2\", \"title\",\"context\",\"text\",\"total_count\",\"total_views\", \"topic_leaf\"]]\n",
    "    else: \n",
    "        sample = sample[[\"url\", \"domain\",\"subdomain\",\"a1\", \"a2\", \"title\",\"context\",\"text\",\"total_count\",\"total_views\", \"topic_leaf\"]]\n",
    "    sample = sample.rename({\"a1\":\"parent_1\", \"a2\":\"parent_2\"},axis = 1)\n",
    "    sample = sample.merge(en_curlie[[\"domain\",\"label\",\"label_leaf\"]], on = \"domain\", how = \"left\")\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efda8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_content_sample = web_content_sample[web_content_sample[\"lang\"] == \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9509f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_harmonic = process_sample(top_pr, harmonic = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c73c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_harmonic.to_csv(\"/scratch/venia/web2wiki/data/iterated_coding/4_top_pr.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972c214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ce71da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sample = process_sample(web_content_sample.sample(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bab2dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sample.to_csv(\"/scratch/venia/web2wiki/data/iterated_coding/4_random_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4951d50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url_count_limited_sample = process_sample(url_count_limited_sample)\n",
    "url_count_limited_sample.to_csv(\"/scratch/venia/web2wiki/data/iterated_coding/one_share_per_url.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3ca464e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the harmonic data\n",
    "harmonic_head = harmonic.head(100).sample(60)\n",
    "harmonic_tail = harmonic.tail(500).sample(60)\n",
    "\n",
    "harmonic_head = process_sample(harmonic_head, harmonic = True)\n",
    "harmonic_tail = process_sample(harmonic_tail, harmonic = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "19fd3db1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "harmonic_head.to_csv(\"/scratch/venia/web2wiki/data/iterated_coding/top_harmonic_sample.csv\", index = False)\n",
    "harmonic_tail.to_csv(\"/scratch/venia/web2wiki/data/iterated_coding/bottom_harmonic_sample.csv\", index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5272cca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8187ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit = explicit_pd.sample(50)\n",
    "explicit = process_sample(explicit_pd, harmonic = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26b89c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "explicit.to_csv(\"/scratch/venia/web2wiki/data/iterated_coding/explicit_mentions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "dc04c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c79e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad33749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
